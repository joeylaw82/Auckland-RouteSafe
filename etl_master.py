import pandas as pd
import geopandas as gpd
import requests
import io
import os
import json
from datetime import datetime
import re
from time import sleep 
from urllib.parse import urlencode 
import sys 

# --- 1. é…ç½®å€ (Configuration) ---
POLICE_DATA_URL = os.environ.get("POLICE_DATA_URL") 
MESHBLOCK_BASE_URL = "https://services.arcgis.com/XTtANUDT8Va4DLwI/arcgis/rest/services/nz_meshblocks/FeatureServer/0"
# æ–°å¢çš„ Area Unit URL
AREA_UNIT_BASE_URL = "https://services2.arcgis.com/vKb0s8tBIA3bdocZ/ArcGIS/rest/services/Area_Unit_2017/FeatureServer/0"
ARCGIS_ROUTES_URL = "https://services2.arcgis.com/JkPEgZJGxhSjYOo0/arcgis/rest/services/BusService/FeatureServer/2/query?where=1%3D1&outFields=*&f=geojson"

AUCKLAND_AUTHORITIES = ['Auckland','Waitemata', 'Counties Manukau', 'Franklin', 'Auckland City'] 

# è¼¸å‡ºæ–‡ä»¶è·¯å¾‘
OUTPUT_DIR = 'data'
OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'route_crime_stats.geojson')
STATS_OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'crime_breakdown.json')
DEBUG_CSV_FILE = os.path.join(OUTPUT_DIR, 'auckland_crime_debug.csv') 

MAX_RECORDS = 2000 # ArcGIS æœå‹™çš„å–®æ¬¡æŸ¥è©¢è¨˜éŒ„é™åˆ¶


# --- 2. è¼”åŠ©å‡½æ•¸ (Helper Functions) ---

def clean_territorial_authority(name: str) -> str:
    """æ¸…ç†è¡Œæ”¿å€åç¨±ã€‚"""
    if pd.isna(name): return ''
    cleaned = re.sub(r'[^\w\s]', '', str(name), flags=re.UNICODE) 
    cleaned = re.sub(r'\s+', ' ', cleaned).strip() 
    return cleaned.upper()

AUCKLAND_AUTHORITIES_CLEANED = [clean_territorial_authority(name) for name in AUCKLAND_AUTHORITIES]


def fetch_arcgis_geometry(base_url: str, id_field: str, out_fields: list) -> gpd.GeoDataFrame:
    """é€šç”¨çš„ ArcGIS åˆ†é ç²å–å¹¾ä½•å‡½æ•¸ã€‚"""
    print(f"   -> æ­£åœ¨ä½¿ç”¨åˆ†é æŠ€è¡“ç²å– {id_field} å¹¾ä½•...")
    
    out_fields_str = ','.join(out_fields)
    count_url = f"{base_url}/query?where=1%3D1&returnCountOnly=true&f=json"
    
    try:
        count_response = requests.get(count_url)
        count_response.raise_for_status()
        total_count = count_response.json().get('count', 0)
        print(f"   -> æœå‹™å ±å‘Šç¸½è¨˜éŒ„æ•¸: {total_count}")
        if total_count == 0:
            print(f"âŒ éŒ¯èª¤: ArcGIS æœå‹™å ±å‘Š {id_field} ç¸½è¨˜éŒ„æ•¸ç‚ºé›¶ã€‚")
            return gpd.GeoDataFrame()
    except Exception as e:
        print(f"âŒ ç²å– {id_field} ç¸½è¨˜éŒ„æ•¸å¤±æ•—: {e}")
        return gpd.GeoDataFrame()

    all_geometry = []
    offset = 0
    
    while offset < total_count:
        print(f"   -> æ­£åœ¨ç²å–æ‰¹æ¬¡ï¼šè¨˜éŒ„ {offset} åˆ° {min(offset + MAX_RECORDS, total_count)}...")
        
        query_params = {
            'where': '1=1',
            'outFields': out_fields_str,
            'resultOffset': offset,
            'resultRecordCount': MAX_RECORDS,
            'f': 'geojson',
            'inSR': '4326', 
            'outSR': '4326',
        }
        
        query_url = f"{base_url}/query?{urlencode(query_params)}"
        
        try:
            response = requests.get(query_url)
            response.raise_for_status()
            
            gdf_batch = gpd.read_file(io.BytesIO(response.content))
            
            if gdf_batch.empty:
                print("   -> ğŸš¨ è­¦å‘Šï¼šArcGIS æœå‹™è¿”å›ç©ºæ‰¹æ¬¡ã€‚åœæ­¢ç²å–ã€‚")
                break
                
            all_geometry.append(gdf_batch)
            offset += len(gdf_batch)
            sleep(0.5) 
            
        except Exception as e:
            print(f"âŒ ç²å–æ‰¹æ¬¡æ•¸æ“šå¤±æ•— (Offset: {offset}): {e}")
            break
            
    if not all_geometry:
        print(f"âŒ éŒ¯èª¤ï¼šæœªèƒ½ç²å–ä»»ä½• {id_field} æ•¸æ“šã€‚")
        return gpd.GeoDataFrame()
        
    gdf_final = pd.concat(all_geometry, ignore_index=True)
    gdf_final = gdf_final[out_fields + ['geometry']].copy()
    
    return gdf_final

def fetch_all_meshblock_geometry(base_url: str) -> gpd.GeoDataFrame:
    """ç²å– Meshblock å¹¾ä½•ã€‚"""
    gdf_final = fetch_arcgis_geometry(base_url, 'MB_number', ['MB_number'])
    if not gdf_final.empty:
        # æ¨™æº–åŒ– MB_number ç‚º 7 ä½å­—ä¸²
        gdf_final['MB_number'] = gdf_final['MB_number'].astype(str).str.strip().str.zfill(7)
        print(f"âœ… æˆåŠŸç²å–æ‰€æœ‰ Meshblock å¹¾ä½•ç¸½è¨˜éŒ„æ•¸: {len(gdf_final)}")
    return gdf_final

def fetch_all_area_unit_geometry(base_url: str) -> gpd.GeoDataFrame:
    """ç²å– Area Unit å¹¾ä½•ã€‚"""
    out_fields = ['AU2017_V1_00', 'AU2017_V1_00_NAME']
    gdf_final = fetch_arcgis_geometry(base_url, 'AU2017_V1_00', out_fields)
    if not gdf_final.empty:
        # Area Unit Code é€šå¸¸æ˜¯ 6 ä½ï¼Œæˆ‘å€‘å°‡å…¶æ¨™æº–åŒ–
        gdf_final['AU_code'] = gdf_final['AU2017_V1_00'].astype(str).str.strip().str.zfill(6)
        gdf_final = gdf_final.rename(columns={'AU2017_V1_00_NAME': 'Area Unit Name'})
        print(f"âœ… æˆåŠŸç²å–æ‰€æœ‰ Area Unit å¹¾ä½•ç¸½è¨˜éŒ„æ•¸: {len(gdf_final)}")
    return gdf_final


def fetch_and_clean_police_data(crime_url: str, meshblock_url: str, area_unit_url: str) -> gpd.GeoDataFrame:
    """ä¸‹è¼‰ã€åˆä½µå’Œç¯©é¸çŠ¯ç½ªæ•¸æ“š (åŒ…å«å…©éšæ®µå¹¾ä½•åŒ¹é…)ã€‚"""
    print("--- 1. æ­£åœ¨è™•ç†è­¦å¯Ÿæ•¸æ“š ---")
    
    # ----------------------------------------------------
    # 1. æ•¸æ“šä¸‹è¼‰å’Œåˆå§‹æ¸…ç† (èˆ‡ä¹‹å‰ç›¸åŒ)
    # ----------------------------------------------------
    print("   -> æ­£åœ¨ä¸‹è¼‰å¤§å‹çŠ¯ç½ªæ•¸æ“šæ–‡ä»¶...")
    try:
        crime_data_response = requests.get(crime_url)
        crime_data_response.raise_for_status()
        df_crime = pd.read_csv(io.BytesIO(crime_data_response.content), encoding='latin1')
        
        # æ ¸å¿ƒæ¬„ä½æ¸…ç†
        df_crime.columns = df_crime.columns.str.strip()
        df_crime.columns = [col.replace('Ã¯Â»Â¿', '').strip() for col in df_crime.columns]
        
        CRIME_MONTH_COL_NAME = 'Year Month'
        if CRIME_MONTH_COL_NAME not in df_crime.columns: raise KeyError(f"æ‰¾ä¸åˆ°å¿…è¦çš„ '{CRIME_MONTH_COL_NAME}' æ¬„ä½ã€‚")
            
        meshblock_cols = [col for col in df_crime.columns if 'meshblock' in col.lower()]
        if 'Meshblock' not in df_crime.columns and meshblock_cols:
            df_crime.rename(columns={meshblock_cols[0]: 'Meshblock'}, inplace=True)
        elif 'Meshblock' not in df_crime.columns:
            raise KeyError(f"æ‰¾ä¸åˆ°å¿…è¦çš„ 'Meshblock' æ¬„ä½ã€‚")
        
        print(f"   -> çŠ¯ç½ªæ•¸æ“šåŸå§‹è¨˜éŒ„æ•¸: {len(df_crime)}") 
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰æˆ–è™•ç†çŠ¯ç½ªæ•¸æ“šå¤±æ•—: {e}")
        raise
    
    # ----------------------------------------------------
    # 2. ç²å–æ‰€æœ‰å¹¾ä½•æ•¸æ“š
    # ----------------------------------------------------
    gdf_meshblocks = fetch_all_meshblock_geometry(meshblock_url)
    gdf_area_units = fetch_all_area_unit_geometry(area_unit_url)
    
    if gdf_meshblocks.empty and gdf_area_units.empty:
        return gpd.GeoDataFrame()

    # æ¨™æº–åŒ–è­¦å¯Ÿæ•¸æ“šçš„ Meshblock ID (7 ä½å­—ä¸²)
    df_crime['Meshblock'] = df_crime['Meshblock'].astype(str).str.strip().str.zfill(7)
    
    # æ‡‰ç”¨ TA æ¸…ç†å‡½æ•¸ä¸¦ç¯©é¸å¥§å…‹è˜­
    df_crime['Territorial Authority Cleaned'] = df_crime['Territorial Authority'].astype(str).apply(clean_territorial_authority)
    df_auckland = df_crime[df_crime['Territorial Authority Cleaned'].isin(AUCKLAND_AUTHORITIES_CLEANED)].copy()
    print(f"   -> å¥§å…‹è˜­TAéæ¿¾å¾Œè¨˜éŒ„æ•¸: {len(df_auckland)}")
    
    # ----------------------------------------------------
    # 3. éšæ®µä¸€ï¼šMeshblock åŒ¹é… (å„ªå…ˆåŒ¹é…)
    # ----------------------------------------------------
    print("   -> åŸ·è¡Œéšæ®µä¸€ï¼šMeshblock å¹¾ä½•åŒ¹é…...")
    df_merged = df_auckland.merge(
        gdf_meshblocks[['MB_number', 'geometry']], 
        left_on='Meshblock', 
        right_on='MB_number', 
        how='left'
    )
    df_merged = df_merged.rename(columns={'geometry': 'geometry_mb'})
    
    unmatched_count_1 = df_merged['geometry_mb'].isna().sum()
    print(f"   -> éšæ®µä¸€ï¼šæˆåŠŸåŒ¹é…è¨˜éŒ„æ•¸: {len(df_merged) - unmatched_count_1}")
    print(f"   -> éšæ®µä¸€ï¼šæœªåŒ¹é…è¨˜éŒ„æ•¸: {unmatched_count_1}")
    
    if unmatched_count_1 > 0 and not gdf_area_units.empty:
        # ----------------------------------------------------
        # 4. éšæ®µäºŒï¼šArea Unit åŒ¹é… (é‡å°æœªåŒ¹é…çš„è¨˜éŒ„)
        # ----------------------------------------------------
        print("   -> åŸ·è¡Œéšæ®µäºŒï¼šå˜—è©¦ä½¿ç”¨ Area Unit å¹¾ä½•åŒ¹é…æœªåŒ¹é…çš„è¨˜éŒ„...")
        
        # æå–æœªåŒ¹é…çš„è¡Œ
        df_unmatched = df_merged[df_merged['geometry_mb'].isna()].copy()
        
        # å‡è¨­ Area Unit Code çš„æ ¼å¼èˆ‡ Meshblock Code ç›¸ä¼¼ï¼Œä½†é•·åº¦ç‚º 6
        # æˆ‘å€‘æˆªæ–· Meshblock IDï¼Œä¸¦å˜—è©¦å°‡å…¶è¦–ç‚º Area Unit Code (AU2017_V1_00)
        df_unmatched['AU_code_match'] = df_unmatched['Meshblock'].str[:6]
        
        df_area_merged = df_unmatched.merge(
            gdf_area_units[['AU_code', 'geometry']],
            left_on='AU_code_match',
            right_on='AU_code',
            how='left'
        )
        df_area_merged = df_area_merged.rename(columns={'geometry': 'geometry_au'})
        
        # å¡«å……ä¸»æ•¸æ“šæ¡†
        df_merged.loc[df_merged['geometry_mb'].isna(), 'geometry_mb'] = df_area_merged['geometry_au'].values
        df_merged = df_merged.rename(columns={'geometry_mb': 'geometry'}) # æœ€çµ‚ä½¿ç”¨çš„å¹¾ä½•æ¬„ä½
        
        unmatched_count_2 = df_merged['geometry'].isna().sum()
        print(f"   -> éšæ®µäºŒï¼šå†æ¬¡æœªåŒ¹é…è¨˜éŒ„æ•¸: {unmatched_count_2}")
        print(f"   -> ç¸½åŒ¹é…æˆåŠŸè¨˜éŒ„æ•¸: {len(df_merged) - unmatched_count_2}")
    else:
        df_merged = df_merged.rename(columns={'geometry_mb': 'geometry'}) # å¦‚æœæ²’æœ‰ç¬¬äºŒéšæ®µï¼Œç›´æ¥é‡å‘½å
        unmatched_count_2 = unmatched_count_1

    # ----------------------------------------------------
    # 5. æ•¸æ“šæ¸…ç†å’Œé™¤éŒ¯è¼¸å‡º
    # ----------------------------------------------------
    
    # è½‰æ›æ™‚é–“æ¬„ä½
    df_merged[CRIME_MONTH_COL_NAME] = pd.to_datetime(
        df_merged[CRIME_MONTH_COL_NAME], 
        format='%Y-%m-%d', 
        errors='coerce' 
    )
    
    df_final = df_merged.copy()

    df_final = df_final.rename(columns={
        'ANZSOC Division': 'OffenceType',     
        'Territorial Authority Cleaned': 'PoliceDistrict', 
        CRIME_MONTH_COL_NAME: 'CrimeMonth'
    })
    
    # è¼¸å‡ºé™¤éŒ¯ CSV (åŒ…å« geometry æ¬„ä½ç‹€æ…‹)
    DEBUG_CSV_FILE = os.path.join(OUTPUT_DIR, 'auckland_crime_debug.csv')
    df_final.drop(columns=['geometry']).to_csv(DEBUG_CSV_FILE, index=False, encoding='utf-8') 
    print(f"âœ… é™¤éŒ¯æ–‡ä»¶ (auckland_crime_debug.csv) è¼¸å‡ºåˆ° {DEBUG_CSV_FILE}")

    # æª¢æŸ¥å’Œåˆªé™¤ç„¡æ•ˆè¡Œ
    missing_geometry_count = df_final['geometry'].isna().sum()
    print(f"   -> ğŸš¨ æª¢æŸ¥: ç¶“éå…©éšæ®µåŒ¹é…å¾Œï¼Œç¼ºå°‘å¹¾ä½•åœ–å½¢çš„è¨˜éŒ„æ•¸: {missing_geometry_count}")
    
    # åˆªé™¤æ²’æœ‰æœ‰æ•ˆå¹¾ä½•åœ–å½¢ã€çŠ¯ç½ªæœˆä»½æˆ–çŠ¯ç½ªé¡å‹çš„è¡Œ
    df_final.dropna(subset=['geometry', 'CrimeMonth', 'OffenceType'], inplace=True)

    print(f"âœ… è­¦å¯Ÿæ•¸æ“šè™•ç†å®Œæˆã€‚æœ€çµ‚ç”¨æ–¼åˆ†æçš„è¨˜éŒ„æ•¸: {len(df_final)}ã€‚")
    if len(df_final) == 0 and len(df_auckland) > 0:
        print("âš ï¸ è­¦å‘Š: æ‰€æœ‰å¥§å…‹è˜­è¨˜éŒ„å‡ç”±æ–¼ç¼ºä¹å¹¾ä½•æˆ–å¿…è¦ä¿¡æ¯è€Œè¢«åˆªé™¤ã€‚")
    
    gdf_crime = gpd.GeoDataFrame(
        df_final.drop(columns=['MB_number', 'Territorial Authority']),
        geometry='geometry', 
        crs="EPSG:4326"
    )
        
    return gdf_crime[['OffenceType', 'PoliceDistrict', 'CrimeMonth', 'geometry']]


# --- 3. ç²å–è·¯ç·šå¹¾ä½• (ä¿æŒä¸è®Š) ---
def fetch_route_geometry() -> gpd.GeoDataFrame:
    """ç²å–å·´å£«è·¯ç·šå¹¾ä½•æ•¸æ“šã€‚"""
    print("--- 2. æ­£åœ¨ç²å– AT è·¯ç·šå¹¾ä½• ---")
    try:
        arcgis_response = requests.get(ARCGIS_ROUTES_URL)
        arcgis_response.raise_for_status() 
        gdf_routes = gpd.read_file(io.BytesIO(arcgis_response.content))
        
        gdf_routes.rename(columns={'ROUTENUMBER': 'Route No'}, inplace=True) 
        gdf_routes = gdf_routes[gdf_routes['MODE'] == 'Bus'].copy()
        gdf_routes = gdf_routes[['Route No', 'geometry']].copy()
        gdf_routes['Route No'] = gdf_routes['Route No'].astype(str)
        
        print(f"âœ… æˆåŠŸç²å– {len(gdf_routes)} æ¢å·´å£«è·¯ç·šå¹¾ä½•ã€‚")
        return gdf_routes
    except Exception as e:
        print(f"âŒ ç²å– ArcGIS æ•¸æ“šå¤±æ•—: {e}")
        raise


# --- 4. ç©ºé–“åˆ†æå’Œæ•¸æ“šå½™ç¸½ (ä¿æŒä¸è®Š) ---

def analyze_and_aggregate(gdf_routes: gpd.GeoDataFrame, gdf_crime: gpd.GeoDataFrame):
    """åŸ·è¡Œç©ºé–“é€£æ¥ã€è¨ˆç®—çµ±è¨ˆæ•¸æ“šä¸¦ç”Ÿæˆ GeoJSON å’Œ JSON æ–‡ä»¶ã€‚"""
    print("--- 3. åŸ·è¡Œç©ºé–“åˆ†æå’Œæ•¸æ“šå½™ç¸½ ---")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True) 
    
    if gdf_crime.empty:
        print("âš ï¸ è­¦å‘Šï¼šç”±æ–¼æ²’æœ‰æœ‰æ•ˆçš„å¥§å…‹è˜­çŠ¯ç½ªæ•¸æ“šï¼Œè·³éç©ºé–“åˆ†æã€‚")
        min_date = 'N/A'
        max_date = 'N/A'
        empty_geojson_output(gdf_routes) 
        empty_stats_output(min_date, max_date)
        return

    # 1. å‰µå»º 50 ç±³ç·©è¡å€
    gdf_routes_proj = gdf_routes.to_crs(epsg=2193) 
    gdf_routes_buffer = gdf_routes_proj.copy()
    gdf_routes_buffer['geometry'] = gdf_routes_buffer.geometry.buffer(50) 
    
    # 2. æŠ•å½±çŠ¯ç½ªæ•¸æ“š
    gdf_crime_proj = gdf_crime.to_crs(epsg=2193)
    
    # 3. ç©ºé–“é€£æ¥ (Spatial Join)
    crime_counts = gpd.sjoin(gdf_crime_proj, gdf_routes_buffer.reset_index(), how='inner', predicate='intersects')
    
    print(f"   -> ç©ºé–“é€£æ¥å¾Œçš„çŠ¯ç½ªäº‹ä»¶è¨˜éŒ„æ•¸: {len(crime_counts)}") 

    if crime_counts.empty:
        print("âš ï¸ è­¦å‘Šï¼šæ²’æœ‰çŠ¯ç½ªäº‹ä»¶è½åœ¨ä»»ä½•å·´å£«è·¯ç·šçš„ 50 ç±³ç·©è¡å€å…§ã€‚")
        min_date = 'N/A'
        max_date = 'N/A'
    else:
        min_date = crime_counts['CrimeMonth'].min().strftime('%Y-%m-%d')
        max_date = crime_counts['CrimeMonth'].max().strftime('%Y-%m-%d')

    # 5. çµ±è¨ˆæ¯æ¢è·¯ç·šçš„çŠ¯ç½ªç¸½æ•¸
    total_crime_summary = crime_counts.groupby('index_right').size().reset_index(name='Total_Crime_Count')
    
    # 6. å½™ç¸½çŠ¯ç½ªç´°ç¯€ (è¶¨å‹¢å’Œé¡å‹)
    crime_details = {
        'metadata': {
            'crime_period_start': min_date,
            'crime_period_end': max_date,
            'buffer_distance_m': 50,
            'data_source': 'NZ Police (Full Available Dataset) merged with NZ Meshblock/Area Unit Geometry'
        },
        'routes': {}
    }
    
    for route_index in total_crime_summary['index_right'].unique():
        route_data = crime_counts[crime_counts['index_right'] == route_index]
        route_no = gdf_routes_buffer.loc[route_index, 'Route No']
        
        monthly_trend = route_data.groupby(route_data['CrimeMonth'].dt.to_period('M')).size().to_dict()
        monthly_trend = {str(k): int(v) for k, v in monthly_trend.items()}
        
        type_breakdown = route_data['OffenceType'].value_counts().to_dict()
        type_breakdown = {k: int(v) for k, v in type_breakdown.items()}
        
        crime_details['routes'][route_no] = {
            'monthly_trend': monthly_trend,
            'type_breakdown': type_breakdown
        }

    # 7. å°‡ç¸½çŠ¯ç½ªè¨ˆæ•¸åˆä½µå›è·¯ç·š GeoDataFrame
    gdf_results = gdf_routes_buffer.reset_index().merge(total_crime_summary, 
                                                        left_on='index', 
                                                        right_on='index_right', 
                                                        how='left')
    gdf_results['Total_Crime_Count'] = gdf_results['Total_Crime_Count'].fillna(0).astype(int)
    gdf_output = gdf_results.to_crs(epsg=4326)[['Route No', 'Total_Crime_Count', 'geometry']].copy()

    # 8. å„²å­˜çµæœ
    gdf_output.to_file(OUTPUT_FILE, driver='GeoJSON', encoding='utf-8')
    print(f"âœ… GeoJSON è¼¸å‡ºåˆ° {OUTPUT_FILE}")
    
    with open(STATS_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(crime_details, f, ensure_ascii=False, indent=4)
    print(f"âœ… çŠ¯ç½ªç´°åˆ†çµ±è¨ˆè¼¸å‡ºåˆ° {STATS_OUTPUT_FILE}")

def empty_geojson_output(gdf_routes):
    # å‰µå»ºä¸€å€‹ç©ºçš„ GeoJSON è¼¸å‡º
    gdf_routes['Total_Crime_Count'] = 0
    gdf_routes = gdf_routes.to_crs(epsg=4326)[['Route No', 'Total_Crime_Count', 'geometry']].copy()
    gdf_routes.to_file(OUTPUT_FILE, driver='GeoJSON', encoding='utf-8')

def empty_stats_output(min_date, max_date):
    # å‰µå»ºä¸€å€‹ç©ºçš„ JSON è¼¸å‡º
    crime_details = {
        'metadata': {
            'crime_period_start': min_date,
            'crime_period_end': max_date,
            'buffer_distance_m': 50,
            'data_source': 'NZ Police (Full Available Dataset) merged with NZ Meshblock/Area Unit Geometry'
        },
        'routes': {}
    }
    with open(STATS_OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(crime_details, f, ensure_ascii=False, indent=4)


# --- 5. ä¸»æµç¨‹ (Main Flow) ---
def run_etl():
    """é‹è¡Œ ETL æµç¨‹ã€‚"""
    if not POLICE_DATA_URL:
        print("âŒ éŒ¯èª¤ï¼šç¼ºå°‘ POLICE_DATA_URL ç’°å¢ƒè®Šé‡ã€‚è«‹åœ¨ GitHub Secrets ä¸­è¨­ç½®ã€‚")
        sys.exit(1)
        
    try:
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # å‚³é Area Unit URL
        gdf_crime = fetch_and_clean_police_data(POLICE_DATA_URL, MESHBLOCK_BASE_URL, AREA_UNIT_BASE_URL) 
        gdf_routes = fetch_route_geometry()
        analyze_and_aggregate(gdf_routes, gdf_crime)
        print("\nğŸ‰ ETL æµç¨‹å…¨éƒ¨æˆåŠŸå®Œæˆï¼")
    except Exception as e:
        error_message = str(e).strip()
        print(f"\nâŒ ETL æµç¨‹ä¸­æ–·: {error_message}")
        sys.exit(1)

if __name__ == "__main__":
    run_etl()
